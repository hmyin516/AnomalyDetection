{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using Tensorflow 2.1.0\n1 GPUs found\nUsing PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print(\"Using Tensorflow {}\".format(tf.__version__))\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"{} GPUs found\".format(len(physical_devices)))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(\"Using {}\".format(physical_devices[0]))\n",
    "\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network hyperparameters from arXiv:1903.02433\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100000 # 500000 in paper\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-5\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.9\n",
    "\n",
    "# Architecture\n",
    "NOISE_DIM = 128\n",
    "\n",
    "# Plotting\n",
    "PREFIX = \"img/{:.0f}D-{}batchsize-\".format(NOISE_DIM, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    \"herwig\": \"GAN-data\\events_anomalydetection_DelphesHerwig_qcd_features.h5\",\n",
    "    \"pythiabg\": \"GAN-data\\events_anomalydetection_DelphesPythia8_v2_qcd_features.h5\",\n",
    "    \"pythiasig\": \"GAN-data\\events_anomalydetection_DelphesPythia8_v2_Wprime_features.h5\"\n",
    "}\n",
    "\n",
    "datatypes = [\"herwig\", \"pythiabg\", \"pythiasig\"]\n",
    "\n",
    "train_features = [\"ptj1\", \"etaj1\", \"mj1\", \"tau21j1\", \"ptj2\", \"etaj2\", \"phij2\", \"mj2\", \"tau21j2\"]\n",
    "# condition_features = [\"mjj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datatype, stop = None, rotate = True, flip_eta = True):\n",
    "    input_frame = pd.read_hdf(filenames[datatype], stop = stop)\n",
    "    output_frame = pd.DataFrame(dtype = \"float32\")\n",
    "\n",
    "    for jet in [\"j1\", \"j2\"]:\n",
    "        output_frame[\"pt\" + jet] = np.sqrt(input_frame[\"px\" + jet]**2 + input_frame[\"py\" + jet]**2)\n",
    "        output_frame[\"eta\" + jet] = np.arcsinh(input_frame[\"pz\" + jet] / output_frame[\"pt\" + jet])\n",
    "        output_frame[\"phi\" + jet] = np.arctan2(input_frame[\"py\" + jet], input_frame[\"px\" + jet])\n",
    "        output_frame[\"m\" + jet] = input_frame[\"m\" + jet]\n",
    "        output_frame[\"p\" + jet] = np.sqrt(input_frame[\"pz\" + jet]**2 + output_frame[\"pt\" + jet]**2)\n",
    "        output_frame[\"e\" + jet] = np.sqrt(output_frame[\"m\" + jet]**2 + output_frame[\"p\" + jet]**2)\n",
    "        output_frame[\"tau21\" + jet] = input_frame[\"tau2\" + jet] / input_frame[\"tau1\" + jet]\n",
    "        output_frame[\"tau32\" + jet] = input_frame[\"tau3\" + jet] / input_frame[\"tau2\" + jet]\n",
    "    \n",
    "    del input_frame\n",
    "    gc.collect()\n",
    "\n",
    "    # Not exact rotation, since negative angles for phi2 are flipped across the x-axis. Should be OK due to symmetry.\n",
    "    if rotate:\n",
    "        output_frame[\"phij2\"] = np.abs(output_frame[\"phij2\"] - output_frame[\"phij1\"])\n",
    "        output_frame[\"phij1\"] = 0\n",
    "    \n",
    "    if flip_eta:\n",
    "        flipped_frame = output_frame.copy()\n",
    "        flipped_frame[\"etaj1\"] *= -1\n",
    "        flipped_frame[\"etaj2\"] *= -1\n",
    "        output_frame = output_frame.append(flipped_frame)\n",
    "        del flipped_frame\n",
    "        gc.collect()\n",
    "    \n",
    "    for jet in [\"j1\", \"j2\"]:\n",
    "        output_frame[\"px\" + jet] = output_frame[\"pt\" + jet] * np.cos(output_frame[\"phi\" + jet])\n",
    "        output_frame[\"py\" + jet] = output_frame[\"pt\" + jet] * np.sin(output_frame[\"phi\" + jet])\n",
    "        output_frame[\"pz\" + jet] = output_frame[\"pt\" + jet] * np.sinh(output_frame[\"eta\" + jet])\n",
    "    \n",
    "    # Dijet properties\n",
    "    output_frame[\"pxjj\"] = output_frame[\"pxj1\"] + output_frame[\"pxj2\"]\n",
    "    output_frame[\"pyjj\"] = output_frame[\"pyj1\"] + output_frame[\"pyj2\"]\n",
    "    output_frame[\"pzjj\"] = output_frame[\"pzj1\"] + output_frame[\"pzj2\"]\n",
    "    output_frame[\"ejj\"] = output_frame[\"ej1\"] + output_frame[\"ej2\"]\n",
    "    output_frame[\"pjj\"] = np.sqrt(output_frame[\"pxjj\"]**2 + output_frame[\"pyjj\"]**2 + output_frame[\"pzjj\"]**2)\n",
    "    output_frame[\"mjj\"] = np.sqrt(output_frame[\"ejj\"]**2 - output_frame[\"pjj\"]**2)\n",
    "\n",
    "    # NaNs may arise from overly sparse jets with tau1 = 0, tau2 = 0, etc.\n",
    "    output_frame.dropna(inplace = True)\n",
    "    output_frame.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return output_frame.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df = load_data(\"herwig\", stop = 10000)\n",
    "else:\n",
    "    df = load_data(\"herwig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset consists of 117 batches of 128 samples each, total 14976 samples\nTestset consists of 39 batches of 128 samples each, total 4992 samples\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# Ensures all batches have same size\n",
    "\n",
    "df.drop([i for i in range(df.shape[0] % (BATCH_SIZE * 4))], inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Normalize all inputs between -1 and 1\n",
    "\n",
    "scaler = MinMaxScaler((-1,1)).fit(df[train_features])\n",
    "feature_df = scaler.transform(df[train_features])\n",
    "\n",
    "X_train, X_test = train_test_split(feature_df, test_size = 0.25, random_state = 1234)\n",
    "len_dataset = int(X_train.shape[0] / BATCH_SIZE)\n",
    "len_testset = int(X_test.shape[0] / BATCH_SIZE)\n",
    "print(\"Dataset consists of {} batches of {} samples each, total {} samples\".format(len_dataset, BATCH_SIZE, len_dataset * BATCH_SIZE))\n",
    "print(\"Testset consists of {} batches of {} samples each, total {} samples\".format(len_testset, BATCH_SIZE, len_testset * BATCH_SIZE))\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(np.array(X_train)).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(np.array(X_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.Input(shape = (NOISE_DIM,)))\n",
    "    model.add(layers.Dense(128, kernel_initializer = 'glorot_uniform'))\n",
    "    model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    \n",
    "    model.add(layers.Reshape((8, 8, 2)))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(32, kernel_size = 2, strides = 1, padding = \"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(16, kernel_size = 3, strides = 1, padding = \"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(len(train_features), activation = 'tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.Input(shape = (len(train_features),)))\n",
    "    model.add(layers.Dense(128))    \n",
    "    model.add(layers.Reshape((8, 8, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(64, kernel_size = 3, strides = 1, padding = \"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(32, kernel_size = 3, strides = 1, padding = \"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "\n",
    "    model.add(layers.Conv2D(16, kernel_size = 3, strides = 1, padding = \"same\"))\n",
    "    model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.LeakyReLU(alpha = 0.2))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_4 (Dense)              (None, 128)               16512     \n_________________________________________________________________\nleaky_re_lu_7 (LeakyReLU)    (None, 128)               0         \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 128)               512       \n_________________________________________________________________\nreshape_2 (Reshape)          (None, 8, 8, 2)           0         \n_________________________________________________________________\nconv2d_transpose_2 (Conv2DTr (None, 8, 8, 32)          288       \n_________________________________________________________________\nleaky_re_lu_8 (LeakyReLU)    (None, 8, 8, 32)          0         \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 8, 8, 32)          128       \n_________________________________________________________________\nconv2d_transpose_3 (Conv2DTr (None, 8, 8, 16)          4624      \n_________________________________________________________________\nleaky_re_lu_9 (LeakyReLU)    (None, 8, 8, 16)          0         \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 8, 8, 16)          64        \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 1024)              0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 9)                 9225      \n=================================================================\nTotal params: 31,353\nTrainable params: 31,001\nNon-trainable params: 352\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_6 (Dense)              (None, 128)               1280      \n_________________________________________________________________\nreshape_3 (Reshape)          (None, 8, 8, 2)           0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 8, 8, 64)          1216      \n_________________________________________________________________\nleaky_re_lu_10 (LeakyReLU)   (None, 8, 8, 64)          0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 8, 8, 32)          18464     \n_________________________________________________________________\nleaky_re_lu_11 (LeakyReLU)   (None, 8, 8, 32)          0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 8, 8, 16)          4624      \n_________________________________________________________________\nleaky_re_lu_12 (LeakyReLU)   (None, 8, 8, 16)          0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 1024)              0         \n_________________________________________________________________\nleaky_re_lu_13 (LeakyReLU)   (None, 1024)              0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 1)                 1025      \n=================================================================\nTotal params: 26,609\nTrainable params: 26,609\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generator_loss(fake_output):\n",
    "    return mse(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensor to float for loss function plotting\n",
    "def K_eval(x):\n",
    "    try:\n",
    "        return K.get_value(K.to_dense(x))\n",
    "    except:\n",
    "        eval_fn = K.function([], [x])\n",
    "        return eval_fn([])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_generator():\n",
    "  gen_input = tf.random.uniform([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_vector = generator(gen_input, training=True)\n",
    "    fake_output = discriminator(generated_vector, training=True)\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "  \n",
    "  return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_discriminator(vectors):\n",
    "  gen_input = tf.random.uniform([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_vector = generator(gen_input, training=True)\n",
    "\n",
    "    real_output = discriminator(vectors, training=True)\n",
    "    fake_output = discriminator(generated_vector, training=True)\n",
    "    \n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "  \n",
    "  return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_generator():\n",
    "  gen_input = tf.random.uniform([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_vector = generator(gen_input, training=False)\n",
    "    fake_output = discriminator(generated_vector, training=False)\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "  \n",
    "  return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_discriminator(vectors):\n",
    "  gen_input = tf.random.uniform([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_vector = generator(gen_input, training=False)\n",
    "\n",
    "    real_output = discriminator(vectors, training=False)\n",
    "    fake_output = discriminator(generated_vector, training=False)\n",
    "    \n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "  \n",
    "  return disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_gan(generator, epoch):\n",
    "    plt.clf()\n",
    "\n",
    "    realdata = scaler.inverse_transform(X_train)\n",
    "    fakedata = scaler.inverse_transform(generator(tf.random.uniform((10000, NOISE_DIM)), training=False))\n",
    "    \n",
    "    f, a = plt.subplots(2, 2, constrained_layout=True)\n",
    "\n",
    "    f.suptitle(\"GAN Generation\")\n",
    "    label = \"Herwig Background\"\n",
    "\n",
    "    a[0, 0].set_title(\"Leading Jet Mass\")\n",
    "    a[0, 0].set_ylabel(\"Normalized to Unity\")\n",
    "    a[0, 0].set_xlabel(\"$m_{J_1}$\")\n",
    "    a[0, 0].hist(realdata[:,2], bins = 25, range = (0, 600), color = \"tab:orange\", alpha = 0.5, label = label, density = True)\n",
    "    a[0, 0].hist(fakedata[:,2], bins = 25, range = (0, 600), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "\n",
    "    a[0, 1].set_title(\"Subleading Jet Mass\")\n",
    "    a[0, 1].set_ylabel(\"Normalized to Unity\")\n",
    "    a[0, 1].set_xlabel(\"$m_{J_2}$\")\n",
    "    a[0, 1].hist(realdata[:,7], bins = 25, range = (0, 600), color = \"tab:orange\", alpha = 0.5, label = label, density = True)\n",
    "    a[0, 1].hist(fakedata[:,7], bins = 25, range = (0, 600), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "    a[0, 1].legend(loc=\"upper right\")\n",
    "\n",
    "    a[1, 0].set_title(\"Leading N-subjettiness ratio\")\n",
    "    a[1, 0].set_ylabel(\"Normalized to Unity\")\n",
    "    a[1, 0].set_xlabel(\"$\\\\tau_{21J_1}$\")\n",
    "    a[1, 0].hist(realdata[:,3], bins = 25, range = (0, 1), color = \"tab:orange\", alpha = 0.5, label = label, density = True)\n",
    "    a[1, 0].hist(fakedata[:,3], bins = 25, range = (0, 1), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "    \n",
    "    a[1, 1].set_title(\"Subleading N-subjettiness ratio\")\n",
    "    a[1, 1].set_ylabel(\"Normalized to Unity\")\n",
    "    a[1, 1].set_xlabel(\"$\\\\tau_{21J_2}$\")\n",
    "    a[1, 1].hist(realdata[:,8], bins = 25, range = (0, 1), color = \"tab:orange\", alpha = 0.5, label = label, density = True)\n",
    "    a[1, 1].hist(fakedata[:,8], bins = 25, range = (0, 1), color = \"tab:blue\", histtype = \"step\", label = \"GAN\", density = True)\n",
    "\n",
    "    if DEBUG:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(\"{}epoch{}-gan.png\".format(PREFIX, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_losses = []\n",
    "train_disc_losses = []\n",
    "test_gen_losses = []\n",
    "test_disc_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_losses(epoch):\n",
    "    plt.clf()\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(2, 1, constrained_layout=True)\n",
    "\n",
    "    f.suptitle(\"Loss Functions\")\n",
    "\n",
    "    ax1.set_title(\"Generator Loss\")\n",
    "    ax1.set_ylabel(\"Wasserstein Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.plot(train_gen_losses, 'b', label = \"Training loss\")\n",
    "    ax1.plot(test_gen_losses, 'r', label = \"Validation loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    ax2.set_title(\"Discriminator Loss\")\n",
    "    ax2.set_ylabel(\"Wasserstein Loss\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.plot(train_disc_losses, 'b', label = \"Training loss\")\n",
    "    ax2.plot(test_disc_losses, 'r', label = \"Validation loss\")\n",
    "\n",
    "    if DEBUG:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(\"{}epoch{}-loss.png\".format(PREFIX, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, testset, epochs):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print_losses = False #((epoch + 1) % 10 == 0)\n",
    "        draw_outputs = ((epoch + 1) % 1000 == 0)\n",
    "\n",
    "        train_gen_loss = 0\n",
    "        train_disc_loss = 0\n",
    "\n",
    "        test_gen_loss = 0\n",
    "        test_disc_loss = 0\n",
    "\n",
    "        # Training\n",
    "\n",
    "        for batchnum, image_batch in enumerate(dataset):\n",
    "            train_disc_loss += K_eval(train_step_discriminator(image_batch)) / len_dataset\n",
    "            train_gen_loss += K_eval(train_step_generator()) / len_dataset\n",
    "\n",
    "\n",
    "        train_gen_losses.append(train_gen_loss)\n",
    "        train_disc_losses.append(train_disc_loss)\n",
    "\n",
    "        # Evaluation\n",
    "\n",
    "        for batchnum, test_batch in enumerate(testset):\n",
    "            test_gen_loss += K_eval(evaluate_generator()) / len_testset\n",
    "            test_disc_loss += K_eval(evaluate_discriminator(test_batch)) / len_testset\n",
    "\n",
    "        test_gen_losses.append(test_gen_loss)\n",
    "        test_disc_losses.append(test_disc_loss)\n",
    "\n",
    "        # Logging\n",
    "\n",
    "        if print_losses:\n",
    "            print()\n",
    "            print(\"Epoch \" + str(epoch + 1) + \":\")\n",
    "            print()\n",
    "            print(\"Generator training loss: \" + str(train_gen_losses[-1]))\n",
    "            print(\"Discriminator training loss: \" + str(train_disc_losses[-1]))\n",
    "            print()\n",
    "            print(\"Generator validation loss: \" + str(test_gen_losses[-1]))\n",
    "            print(\"Discriminator validation loss: \" + str(test_disc_losses[-1]))\n",
    "\n",
    "        if draw_outputs:\n",
    "            print()\n",
    "            print(\"Epoch \" + str(epoch + 1) + \":\")\n",
    "            graph_gan(generator, epoch + 1)\n",
    "            graph_losses(epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 79/100000 [02:21<49:51:19,  1.80s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-82c61bd80bdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-51-2cac71679d25>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, testset, epochs)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatchnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mtrain_disc_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mK_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step_discriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mtrain_gen_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mK_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ad\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ad\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ad\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ad\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ad\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ad\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\anaconda3\\envs\\ad\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_dataset, test_dataset, EPOCHS)"
   ]
  }
 ]
}