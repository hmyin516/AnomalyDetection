{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIM = 128 # 64 in Gitlab\n",
    "TESTING= False\n",
    "BATCH_SIZE = 64\n",
    "SAMPLE_SIZE= 50000\n",
    "BINS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = {\n",
    "    \"herwig\": \"../data/events_anomalydetection_DelphesHerwig_qcd_features.h5\",\n",
    "    \"pythiabg\": \"../data/events_anomalydetection_DelphesPythia8_v2_qcd_features.h5\",\n",
    "    \"pythiasig\": \"../data/events_anomalydetection_DelphesPythia8_v2_Wprime_features.h5\"\n",
    "}\n",
    "\n",
    "datatypes = [\"herwig\", \"pythiabg\", \"pythiasig\"]\n",
    "\n",
    "train_features = [\"ptj1\", \"etaj1\", \"mj1\", \"ptj2\", \"etaj2\", \"phij2\", \"mj2\", \"tau21j1\", \"tau21j2\"]\n",
    "condition_features = [\"mjj\"]\n",
    "\n",
    "features = train_features + condition_features\n",
    "GEN_DIM = NOISE_DIM + len(condition_features)\n",
    "DISC_DIM = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cut_data(uncut_data, pTmin = 1200, etamax = 2.5):\n",
    "    # Column 0: ptj1\n",
    "    # Column 1: etaj1\n",
    "    # Column 3: ptj2\n",
    "    # Column 4: etaj2\n",
    "    return uncut_data[((uncut_data[:,0] > pTmin) & (np.abs(uncut_data[:,1]) < etamax)) | ((uncut_data[:,3] > pTmin) & (np.abs(uncut_data[:,4]) < etamax))]\n",
    "\n",
    "np_bg_SB = np.load('../data/processed/np_bg_SB_2.npy')\n",
    "np_bg_SR = np.load('../data/processed/np_bg_SR_2.npy')\n",
    "np_sig_SR = np.load('../data/processed/np_sig_SR_2.npy')\n",
    "\n",
    "np_sig_SR_labeled = np.copy(np_sig_SR)\n",
    "np_bg_SR_labeled = np.copy(np_bg_SR)\n",
    "\n",
    "np_sig_SR_labeled = np.append(np_sig_SR_labeled,np.ones([len(np_sig_SR),1]),1)\n",
    "np_bg_SR_labeled = np.append(np_bg_SR_labeled,np.zeros([len(np_bg_SR),1]),1)\n",
    "np_combined_SR = np.concatenate((np_bg_SR, np_sig_SR), axis = 0)\n",
    "np_combined_SR_labeled = np.concatenate((np_sig_SR_labeled,np_bg_SR_labeled),axis=0)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6205640442400567"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_sig_SR.shape[0]/np_bg_SR.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "gen_model = tf.keras.models.load_model('../Results/cdijetgan/saverun4/models/epoch1000-generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gan(generator, realdata):\n",
    "\n",
    "\n",
    "    labels = sample_fake(refdata = realdata, size = SAMPLE_SIZE) # Sample mjj from the existing distribution of mjj for comparison\n",
    "    labels_scaled = scaler_mjj.transform(labels.reshape(-1,1))\n",
    "    \n",
    "    fakedata_uncut_unscaled = generator(tf.concat([tf.random.uniform((SAMPLE_SIZE, NOISE_DIM)), labels_scaled], 1), training=False)\n",
    "    fakedata_uncut = np.concatenate((scaler.inverse_transform(fakedata_uncut_unscaled), labels.reshape(-1,1)), axis = 1)\n",
    "   \n",
    "\n",
    "    # At least one jet has pT > 1200 and |eta| < 2.5\n",
    "    fakedata = cut_data(fakedata_uncut)\n",
    "\n",
    "    # mjj = sqrt(Ejj**2 - pxjj**2 - pyjj**2 - pzjj**2)\n",
    "    fakedata_mjj = mjj(fakedata)\n",
    "\n",
    "    return fakedata\n",
    "def mjj(output):\n",
    "    pt1 = output[:,0]\n",
    "    eta1 = output[:,1]\n",
    "    m1 = output[:,2]\n",
    "    pt2 = output[:,3]\n",
    "    eta2 = output[:,4]\n",
    "    phi2 = output[:,5]\n",
    "    m2 = output[:,6]\n",
    "    ejj = np.sqrt((pt1 * np.cosh(eta1))**2 + m1**2) + np.sqrt((pt2 * np.cosh(eta2))**2 + m2**2)\n",
    "    pxjj = pt1 + pt2 * np.cos(phi2)\n",
    "    pyjj = pt2 * np.sin(phi2)\n",
    "    pzjj = pt1 * np.sinh(eta1) + pt2 * np.sinh(eta2)\n",
    "    return np.sqrt(ejj**2 - pxjj**2 - pyjj**2 - pzjj**2)\n",
    "def sample_fake(refdata = np_bg_SR, size = BATCH_SIZE):\n",
    "    rand_idx = np.random.choice(refdata.shape[0], size = size)\n",
    "    return refdata[rand_idx, -1].reshape((-1,1))\n",
    "def sample_data(refdata = np_combined_SR_labeled,size= 10000):\n",
    "    rand_idx = np.random.choice(refdata.shape[0], size)\n",
    "    return refdata[rand_idx, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = sample_data(size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_bg_SB_trimmed = np.delete(np_bg_SB, [i for i in range(np_bg_SB.shape[0] % (BATCH_SIZE * 4))], axis = 0)\n",
    "\n",
    "# Normalize inputs between -1 and 1, mjj between 0 and 1\n",
    "scaler = MinMaxScaler((-1,1)).fit(np_bg_SB_trimmed[:,:-1])\n",
    "scaler_mjj = MinMaxScaler((0,1)).fit(np_bg_SB_trimmed[:,-1].reshape(-1,1))\n",
    "np_bg_SB_scaled = np.concatenate((scaler.transform(np_bg_SB_trimmed[:,:-1]), scaler_mjj.transform(np_bg_SB_trimmed[:,-1].reshape(-1,1))), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = generate_gan(gen_model,np_combined_SR)\n",
    "generated_data2 = generate_gan(gen_model,np_combined_SR)\n",
    "generated_data = np.concatenate((generated_data,generated_data2), axis = 0)\n",
    "generated_data_labeled  = np.copy(generated_data)\n",
    "generated_data_labeled = np.append(generated_data_labeled,np.zeros([len(generated_data_labeled),1]),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_bg_SR_labeled  = np.copy(np_bg_SR)\n",
    "np_bg_SR_labeled  = np.append(np_bg_SR_labeled ,np.zeros([len(np_bg_SR_labeled) ,1]),1)\n",
    "np_sig_SR_labeled  = np.copy(np_sig_SR)\n",
    "np_sig_SR_labeled  = np.append(np_sig_SR_labeled ,np.ones([len(np_sig_SR_labeled) ,1]),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_df = pd.DataFrame(generated_data_labeled, columns = ['pt1','eta1','m1','pt2','eta2','phi2','m2','tau21j1','tau21j2','mjj','sblabel'])\n",
    "np_sig_df = pd.DataFrame(np_sig_SR_labeled, columns = ['pt1','eta1','m1','pt2','eta2','phi2','m2','tau21j1','tau21j2','mjj','sblabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt1</th>\n",
       "      <th>eta1</th>\n",
       "      <th>m1</th>\n",
       "      <th>pt2</th>\n",
       "      <th>eta2</th>\n",
       "      <th>phi2</th>\n",
       "      <th>m2</th>\n",
       "      <th>tau21j1</th>\n",
       "      <th>tau21j2</th>\n",
       "      <th>mjj</th>\n",
       "      <th>sblabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1240.366674</td>\n",
       "      <td>0.380573</td>\n",
       "      <td>209.133614</td>\n",
       "      <td>1071.856886</td>\n",
       "      <td>-1.536889</td>\n",
       "      <td>2.652903</td>\n",
       "      <td>242.083373</td>\n",
       "      <td>0.835362</td>\n",
       "      <td>0.603953</td>\n",
       "      <td>3433.415527</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1733.909712</td>\n",
       "      <td>0.567251</td>\n",
       "      <td>133.514546</td>\n",
       "      <td>1793.047707</td>\n",
       "      <td>0.909862</td>\n",
       "      <td>3.119892</td>\n",
       "      <td>621.439472</td>\n",
       "      <td>0.950038</td>\n",
       "      <td>0.378180</td>\n",
       "      <td>3579.931885</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1382.459401</td>\n",
       "      <td>0.814356</td>\n",
       "      <td>132.657246</td>\n",
       "      <td>403.665405</td>\n",
       "      <td>-2.056947</td>\n",
       "      <td>3.723075</td>\n",
       "      <td>76.922235</td>\n",
       "      <td>0.594219</td>\n",
       "      <td>0.767021</td>\n",
       "      <td>3343.528564</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1220.226854</td>\n",
       "      <td>1.309399</td>\n",
       "      <td>253.786699</td>\n",
       "      <td>962.904241</td>\n",
       "      <td>-0.826233</td>\n",
       "      <td>2.648015</td>\n",
       "      <td>29.393383</td>\n",
       "      <td>0.258914</td>\n",
       "      <td>0.748940</td>\n",
       "      <td>3499.114014</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1777.963687</td>\n",
       "      <td>0.535396</td>\n",
       "      <td>509.312761</td>\n",
       "      <td>1794.679253</td>\n",
       "      <td>0.082154</td>\n",
       "      <td>3.199875</td>\n",
       "      <td>109.566839</td>\n",
       "      <td>0.723776</td>\n",
       "      <td>0.446367</td>\n",
       "      <td>3603.340088</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1484.124208</td>\n",
       "      <td>0.292365</td>\n",
       "      <td>708.086713</td>\n",
       "      <td>1311.044494</td>\n",
       "      <td>-1.061775</td>\n",
       "      <td>3.469861</td>\n",
       "      <td>186.629306</td>\n",
       "      <td>0.144988</td>\n",
       "      <td>0.478857</td>\n",
       "      <td>3442.107666</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1325.472651</td>\n",
       "      <td>-1.170568</td>\n",
       "      <td>234.121937</td>\n",
       "      <td>1167.178912</td>\n",
       "      <td>0.676366</td>\n",
       "      <td>3.202126</td>\n",
       "      <td>72.978088</td>\n",
       "      <td>0.634627</td>\n",
       "      <td>0.688800</td>\n",
       "      <td>3617.594238</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1592.256869</td>\n",
       "      <td>0.791757</td>\n",
       "      <td>289.553807</td>\n",
       "      <td>1585.246045</td>\n",
       "      <td>-0.156222</td>\n",
       "      <td>3.144207</td>\n",
       "      <td>37.598600</td>\n",
       "      <td>0.526267</td>\n",
       "      <td>0.476040</td>\n",
       "      <td>3439.064697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1432.921196</td>\n",
       "      <td>-0.038133</td>\n",
       "      <td>562.863361</td>\n",
       "      <td>1292.820076</td>\n",
       "      <td>-1.541369</td>\n",
       "      <td>3.208955</td>\n",
       "      <td>57.064643</td>\n",
       "      <td>0.542999</td>\n",
       "      <td>0.598808</td>\n",
       "      <td>3543.805908</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1679.348201</td>\n",
       "      <td>0.786767</td>\n",
       "      <td>323.242934</td>\n",
       "      <td>1692.232652</td>\n",
       "      <td>0.124537</td>\n",
       "      <td>3.185204</td>\n",
       "      <td>140.520917</td>\n",
       "      <td>0.629468</td>\n",
       "      <td>0.747610</td>\n",
       "      <td>3448.274170</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pt1      eta1          m1          pt2      eta2      phi2  \\\n",
       "0      1240.366674  0.380573  209.133614  1071.856886 -1.536889  2.652903   \n",
       "1      1733.909712  0.567251  133.514546  1793.047707  0.909862  3.119892   \n",
       "2      1382.459401  0.814356  132.657246   403.665405 -2.056947  3.723075   \n",
       "3      1220.226854  1.309399  253.786699   962.904241 -0.826233  2.648015   \n",
       "4      1777.963687  0.535396  509.312761  1794.679253  0.082154  3.199875   \n",
       "...            ...       ...         ...          ...       ...       ...   \n",
       "99995  1484.124208  0.292365  708.086713  1311.044494 -1.061775  3.469861   \n",
       "99996  1325.472651 -1.170568  234.121937  1167.178912  0.676366  3.202126   \n",
       "99997  1592.256869  0.791757  289.553807  1585.246045 -0.156222  3.144207   \n",
       "99998  1432.921196 -0.038133  562.863361  1292.820076 -1.541369  3.208955   \n",
       "99999  1679.348201  0.786767  323.242934  1692.232652  0.124537  3.185204   \n",
       "\n",
       "               m2   tau21j1   tau21j2          mjj  sblabel  \n",
       "0      242.083373  0.835362  0.603953  3433.415527      0.0  \n",
       "1      621.439472  0.950038  0.378180  3579.931885      0.0  \n",
       "2       76.922235  0.594219  0.767021  3343.528564      0.0  \n",
       "3       29.393383  0.258914  0.748940  3499.114014      0.0  \n",
       "4      109.566839  0.723776  0.446367  3603.340088      0.0  \n",
       "...           ...       ...       ...          ...      ...  \n",
       "99995  186.629306  0.144988  0.478857  3442.107666      0.0  \n",
       "99996   72.978088  0.634627  0.688800  3617.594238      0.0  \n",
       "99997   37.598600  0.526267  0.476040  3439.064697      0.0  \n",
       "99998   57.064643  0.542999  0.598808  3543.805908      0.0  \n",
       "99999  140.520917  0.629468  0.747610  3448.274170      0.0  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt1</th>\n",
       "      <th>eta1</th>\n",
       "      <th>m1</th>\n",
       "      <th>pt2</th>\n",
       "      <th>eta2</th>\n",
       "      <th>phi2</th>\n",
       "      <th>m2</th>\n",
       "      <th>tau21j1</th>\n",
       "      <th>tau21j2</th>\n",
       "      <th>mjj</th>\n",
       "      <th>sblabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1914.942993</td>\n",
       "      <td>0.369530</td>\n",
       "      <td>105.035004</td>\n",
       "      <td>1583.804443</td>\n",
       "      <td>-0.185737</td>\n",
       "      <td>2.898982</td>\n",
       "      <td>461.574005</td>\n",
       "      <td>0.552809</td>\n",
       "      <td>0.121353</td>\n",
       "      <td>3662.211182</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1684.598755</td>\n",
       "      <td>-0.523116</td>\n",
       "      <td>159.865997</td>\n",
       "      <td>1647.186768</td>\n",
       "      <td>0.110357</td>\n",
       "      <td>3.141156</td>\n",
       "      <td>514.883972</td>\n",
       "      <td>0.440781</td>\n",
       "      <td>0.299984</td>\n",
       "      <td>3586.710693</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1789.997070</td>\n",
       "      <td>0.156652</td>\n",
       "      <td>93.665901</td>\n",
       "      <td>1569.509399</td>\n",
       "      <td>0.144243</td>\n",
       "      <td>3.235663</td>\n",
       "      <td>475.316986</td>\n",
       "      <td>0.136103</td>\n",
       "      <td>0.135523</td>\n",
       "      <td>3421.777344</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1672.631348</td>\n",
       "      <td>-1.015185</td>\n",
       "      <td>116.327003</td>\n",
       "      <td>1568.322998</td>\n",
       "      <td>-0.350886</td>\n",
       "      <td>3.165926</td>\n",
       "      <td>561.236023</td>\n",
       "      <td>0.617014</td>\n",
       "      <td>0.294746</td>\n",
       "      <td>3536.982910</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1431.694946</td>\n",
       "      <td>-0.700751</td>\n",
       "      <td>513.015991</td>\n",
       "      <td>1099.721313</td>\n",
       "      <td>0.945019</td>\n",
       "      <td>3.245961</td>\n",
       "      <td>108.752998</td>\n",
       "      <td>0.183145</td>\n",
       "      <td>0.456454</td>\n",
       "      <td>3481.573486</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150591</th>\n",
       "      <td>1678.012939</td>\n",
       "      <td>0.827268</td>\n",
       "      <td>473.352997</td>\n",
       "      <td>1653.355347</td>\n",
       "      <td>0.978250</td>\n",
       "      <td>3.045114</td>\n",
       "      <td>111.844002</td>\n",
       "      <td>0.090573</td>\n",
       "      <td>0.308552</td>\n",
       "      <td>3409.779297</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150592</th>\n",
       "      <td>1741.585083</td>\n",
       "      <td>-0.203934</td>\n",
       "      <td>96.165001</td>\n",
       "      <td>1728.791870</td>\n",
       "      <td>0.121508</td>\n",
       "      <td>3.133633</td>\n",
       "      <td>472.475006</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.157020</td>\n",
       "      <td>3581.979492</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150593</th>\n",
       "      <td>1289.501831</td>\n",
       "      <td>0.922850</td>\n",
       "      <td>115.719002</td>\n",
       "      <td>1153.867065</td>\n",
       "      <td>-0.919407</td>\n",
       "      <td>3.193555</td>\n",
       "      <td>489.053009</td>\n",
       "      <td>0.271544</td>\n",
       "      <td>0.203001</td>\n",
       "      <td>3622.836914</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150594</th>\n",
       "      <td>1787.707764</td>\n",
       "      <td>0.032824</td>\n",
       "      <td>508.045013</td>\n",
       "      <td>1381.171143</td>\n",
       "      <td>0.933776</td>\n",
       "      <td>3.163839</td>\n",
       "      <td>91.104897</td>\n",
       "      <td>0.166132</td>\n",
       "      <td>0.588186</td>\n",
       "      <td>3546.809082</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150595</th>\n",
       "      <td>1745.240479</td>\n",
       "      <td>0.112269</td>\n",
       "      <td>114.938004</td>\n",
       "      <td>1705.561890</td>\n",
       "      <td>-0.288385</td>\n",
       "      <td>3.096745</td>\n",
       "      <td>553.737000</td>\n",
       "      <td>0.153972</td>\n",
       "      <td>0.524699</td>\n",
       "      <td>3607.571045</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150596 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                pt1      eta1          m1          pt2      eta2      phi2  \\\n",
       "0       1914.942993  0.369530  105.035004  1583.804443 -0.185737  2.898982   \n",
       "1       1684.598755 -0.523116  159.865997  1647.186768  0.110357  3.141156   \n",
       "2       1789.997070  0.156652   93.665901  1569.509399  0.144243  3.235663   \n",
       "3       1672.631348 -1.015185  116.327003  1568.322998 -0.350886  3.165926   \n",
       "4       1431.694946 -0.700751  513.015991  1099.721313  0.945019  3.245961   \n",
       "...             ...       ...         ...          ...       ...       ...   \n",
       "150591  1678.012939  0.827268  473.352997  1653.355347  0.978250  3.045114   \n",
       "150592  1741.585083 -0.203934   96.165001  1728.791870  0.121508  3.133633   \n",
       "150593  1289.501831  0.922850  115.719002  1153.867065 -0.919407  3.193555   \n",
       "150594  1787.707764  0.032824  508.045013  1381.171143  0.933776  3.163839   \n",
       "150595  1745.240479  0.112269  114.938004  1705.561890 -0.288385  3.096745   \n",
       "\n",
       "                m2   tau21j1   tau21j2          mjj  sblabel  \n",
       "0       461.574005  0.552809  0.121353  3662.211182      1.0  \n",
       "1       514.883972  0.440781  0.299984  3586.710693      1.0  \n",
       "2       475.316986  0.136103  0.135523  3421.777344      1.0  \n",
       "3       561.236023  0.617014  0.294746  3536.982910      1.0  \n",
       "4       108.752998  0.183145  0.456454  3481.573486      1.0  \n",
       "...            ...       ...       ...          ...      ...  \n",
       "150591  111.844002  0.090573  0.308552  3409.779297      1.0  \n",
       "150592  472.475006  0.202213  0.157020  3581.979492      1.0  \n",
       "150593  489.053009  0.271544  0.203001  3622.836914      1.0  \n",
       "150594   91.104897  0.166132  0.588186  3546.809082      1.0  \n",
       "150595  553.737000  0.153972  0.524699  3607.571045      1.0  \n",
       "\n",
       "[150596 rows x 11 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_sig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      \n",
    "]\n",
    "def classifier_model():    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(64,input_dim = 4,activation ='relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(64,activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(64,activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(64,activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=[METRICS])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 64)                320       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 12,865\n",
      "Trainable params: 12,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier_model()\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def learningCurveLoss(history):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.plot(history.history['loss'], linewidth=1)\n",
    "    plt.plot(history.history['val_loss'], linewidth=1)\n",
    "    plt.title('Model Loss over Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    #plt.ylim(0,5)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['training sample loss','validation sample loss'])\n",
    "    plt.savefig('5_tag_learning_curve.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    #plt.savefig(\"Learning_Curve\")\n",
    "def plot_roc_curve(y_test, y_test_score):\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_score)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr,label=' AUC = %.1f%%'%(auc_value*100.))\n",
    "    plt.plot([0, 1], [0, 1], 'k-')\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split background in SR into testing and training. Testing is to inject signals\n",
    "training_idx = np.random.randint(np_bg_SR_labeled.shape[0], size=(int)(np_bg_SR_labeled.shape[0]/2))\n",
    "test_idx = np.random.randint(np_bg_SR_labeled.shape[0], size=(int)(np_bg_SR_labeled.shape[0]/2))\n",
    "training, test = np_bg_SR_labeled[training_idx,:], np_bg_SR_labeled[test_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121338, 11)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other half of the bg combined with FULL signal\n",
    "testing_sample = np.concatenate((np_sig_SR_labeled,test),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271934, 11)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = testing_sample[:,[2,6,7,8]]\n",
    "sby_test = testing_sample[:,-1]\n",
    "rfy_test = np.ones([len(x_test),1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271934, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_df = pd.DataFrame(testing_sample, columns = ['pt1','eta1','m1','pt2','eta2','phi2','m2','tau21j1','tau21j2','mjj','sblabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt1</th>\n",
       "      <th>eta1</th>\n",
       "      <th>m1</th>\n",
       "      <th>pt2</th>\n",
       "      <th>eta2</th>\n",
       "      <th>phi2</th>\n",
       "      <th>m2</th>\n",
       "      <th>tau21j1</th>\n",
       "      <th>tau21j2</th>\n",
       "      <th>mjj</th>\n",
       "      <th>sblabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1914.942993</td>\n",
       "      <td>0.369530</td>\n",
       "      <td>105.035004</td>\n",
       "      <td>1583.804443</td>\n",
       "      <td>-0.185737</td>\n",
       "      <td>2.898982</td>\n",
       "      <td>461.574005</td>\n",
       "      <td>0.552809</td>\n",
       "      <td>0.121353</td>\n",
       "      <td>3662.211182</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1684.598755</td>\n",
       "      <td>-0.523116</td>\n",
       "      <td>159.865997</td>\n",
       "      <td>1647.186768</td>\n",
       "      <td>0.110357</td>\n",
       "      <td>3.141156</td>\n",
       "      <td>514.883972</td>\n",
       "      <td>0.440781</td>\n",
       "      <td>0.299984</td>\n",
       "      <td>3586.710693</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1789.997070</td>\n",
       "      <td>0.156652</td>\n",
       "      <td>93.665901</td>\n",
       "      <td>1569.509399</td>\n",
       "      <td>0.144243</td>\n",
       "      <td>3.235663</td>\n",
       "      <td>475.316986</td>\n",
       "      <td>0.136103</td>\n",
       "      <td>0.135523</td>\n",
       "      <td>3421.777344</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1672.631348</td>\n",
       "      <td>-1.015185</td>\n",
       "      <td>116.327003</td>\n",
       "      <td>1568.322998</td>\n",
       "      <td>-0.350886</td>\n",
       "      <td>3.165926</td>\n",
       "      <td>561.236023</td>\n",
       "      <td>0.617014</td>\n",
       "      <td>0.294746</td>\n",
       "      <td>3536.982910</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1431.694946</td>\n",
       "      <td>-0.700751</td>\n",
       "      <td>513.015991</td>\n",
       "      <td>1099.721313</td>\n",
       "      <td>0.945019</td>\n",
       "      <td>3.245961</td>\n",
       "      <td>108.752998</td>\n",
       "      <td>0.183145</td>\n",
       "      <td>0.456454</td>\n",
       "      <td>3481.573486</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271929</th>\n",
       "      <td>1330.065552</td>\n",
       "      <td>0.725246</td>\n",
       "      <td>93.252701</td>\n",
       "      <td>1166.610840</td>\n",
       "      <td>-0.853586</td>\n",
       "      <td>2.880588</td>\n",
       "      <td>125.022003</td>\n",
       "      <td>0.196604</td>\n",
       "      <td>0.592510</td>\n",
       "      <td>3300.257568</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271930</th>\n",
       "      <td>1692.535278</td>\n",
       "      <td>1.062261</td>\n",
       "      <td>64.202301</td>\n",
       "      <td>1353.070679</td>\n",
       "      <td>-0.120157</td>\n",
       "      <td>3.845955</td>\n",
       "      <td>50.237099</td>\n",
       "      <td>0.662372</td>\n",
       "      <td>0.813501</td>\n",
       "      <td>3417.159668</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271931</th>\n",
       "      <td>1251.433838</td>\n",
       "      <td>-1.294582</td>\n",
       "      <td>524.963013</td>\n",
       "      <td>1126.043213</td>\n",
       "      <td>0.616599</td>\n",
       "      <td>3.082296</td>\n",
       "      <td>257.658997</td>\n",
       "      <td>0.431158</td>\n",
       "      <td>0.658276</td>\n",
       "      <td>3628.140137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271932</th>\n",
       "      <td>1690.855713</td>\n",
       "      <td>0.147932</td>\n",
       "      <td>102.100998</td>\n",
       "      <td>1308.839844</td>\n",
       "      <td>-0.763140</td>\n",
       "      <td>3.279803</td>\n",
       "      <td>265.971008</td>\n",
       "      <td>0.362470</td>\n",
       "      <td>0.747445</td>\n",
       "      <td>3307.515625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271933</th>\n",
       "      <td>1579.060913</td>\n",
       "      <td>-0.369966</td>\n",
       "      <td>665.502991</td>\n",
       "      <td>1557.297607</td>\n",
       "      <td>0.242007</td>\n",
       "      <td>3.259644</td>\n",
       "      <td>311.428009</td>\n",
       "      <td>0.433836</td>\n",
       "      <td>0.701272</td>\n",
       "      <td>3436.001709</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271934 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                pt1      eta1          m1          pt2      eta2      phi2  \\\n",
       "0       1914.942993  0.369530  105.035004  1583.804443 -0.185737  2.898982   \n",
       "1       1684.598755 -0.523116  159.865997  1647.186768  0.110357  3.141156   \n",
       "2       1789.997070  0.156652   93.665901  1569.509399  0.144243  3.235663   \n",
       "3       1672.631348 -1.015185  116.327003  1568.322998 -0.350886  3.165926   \n",
       "4       1431.694946 -0.700751  513.015991  1099.721313  0.945019  3.245961   \n",
       "...             ...       ...         ...          ...       ...       ...   \n",
       "271929  1330.065552  0.725246   93.252701  1166.610840 -0.853586  2.880588   \n",
       "271930  1692.535278  1.062261   64.202301  1353.070679 -0.120157  3.845955   \n",
       "271931  1251.433838 -1.294582  524.963013  1126.043213  0.616599  3.082296   \n",
       "271932  1690.855713  0.147932  102.100998  1308.839844 -0.763140  3.279803   \n",
       "271933  1579.060913 -0.369966  665.502991  1557.297607  0.242007  3.259644   \n",
       "\n",
       "                m2   tau21j1   tau21j2          mjj  sblabel  \n",
       "0       461.574005  0.552809  0.121353  3662.211182      1.0  \n",
       "1       514.883972  0.440781  0.299984  3586.710693      1.0  \n",
       "2       475.316986  0.136103  0.135523  3421.777344      1.0  \n",
       "3       561.236023  0.617014  0.294746  3536.982910      1.0  \n",
       "4       108.752998  0.183145  0.456454  3481.573486      1.0  \n",
       "...            ...       ...       ...          ...      ...  \n",
       "271929  125.022003  0.196604  0.592510  3300.257568      0.0  \n",
       "271930   50.237099  0.662372  0.813501  3417.159668      0.0  \n",
       "271931  257.658997  0.431158  0.658276  3628.140137      0.0  \n",
       "271932  265.971008  0.362470  0.747445  3307.515625      0.0  \n",
       "271933  311.428009  0.433836  0.701272  3436.001709      0.0  \n",
       "\n",
       "[271934 rows x 11 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 121338, 1.0: 150596}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_sample[:,10]\n",
    "unique, counts = np.unique(testing_sample[:,10], return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_ratio = np.logspace(-3,-0.205,5)\n",
    "#sb_ratio = np.linspace(0,0.62,10)\n",
    "mixedsb = []\n",
    "generated_data = []\n",
    "for i in sb_ratio:\n",
    "    sampled_signal = np.random.choice(np_sig_SR_labeled.shape[0], (int)(i * training.shape[0]))\n",
    "    combined = np.concatenate((np_sig_SR_labeled[sampled_signal,:],training), axis =0)\n",
    "    gen = generate_gan(gen_model,combined)\n",
    "    gen2 = generate_gan(gen_model,combined)\n",
    "    gen_data = np.concatenate((gen,gen2),axis=0)\n",
    "    generated_data_labeled  = np.copy(gen_data)\n",
    "    generated_data_labeled = np.append(generated_data_labeled,np.zeros([len(generated_data_labeled),1]),1)\n",
    "    mixedsb.append(sample_data(combined,100000))\n",
    "    generated_data.append(generated_data_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_label_df = pd.DataFrame(mixedsb[4], columns = ['pt1','eta1','m1','pt2','eta2','phi2','m2','tau21j1','tau21j2','mjj','sblabel'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pt1</th>\n",
       "      <th>eta1</th>\n",
       "      <th>m1</th>\n",
       "      <th>pt2</th>\n",
       "      <th>eta2</th>\n",
       "      <th>phi2</th>\n",
       "      <th>m2</th>\n",
       "      <th>tau21j1</th>\n",
       "      <th>tau21j2</th>\n",
       "      <th>mjj</th>\n",
       "      <th>sblabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1445.309570</td>\n",
       "      <td>-0.370347</td>\n",
       "      <td>57.344601</td>\n",
       "      <td>1356.463745</td>\n",
       "      <td>1.144163</td>\n",
       "      <td>3.145604</td>\n",
       "      <td>389.571991</td>\n",
       "      <td>0.589472</td>\n",
       "      <td>0.445159</td>\n",
       "      <td>3677.823730</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1626.900635</td>\n",
       "      <td>0.364539</td>\n",
       "      <td>103.980003</td>\n",
       "      <td>1536.651489</td>\n",
       "      <td>-0.616633</td>\n",
       "      <td>3.125390</td>\n",
       "      <td>384.050995</td>\n",
       "      <td>0.522831</td>\n",
       "      <td>0.283367</td>\n",
       "      <td>3593.491455</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1752.570679</td>\n",
       "      <td>-0.254160</td>\n",
       "      <td>480.951996</td>\n",
       "      <td>1661.777588</td>\n",
       "      <td>-0.784472</td>\n",
       "      <td>3.216936</td>\n",
       "      <td>107.120003</td>\n",
       "      <td>0.570297</td>\n",
       "      <td>0.358381</td>\n",
       "      <td>3605.607422</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1547.765137</td>\n",
       "      <td>-0.299859</td>\n",
       "      <td>205.778000</td>\n",
       "      <td>1436.573853</td>\n",
       "      <td>0.674742</td>\n",
       "      <td>3.127375</td>\n",
       "      <td>235.145004</td>\n",
       "      <td>0.826377</td>\n",
       "      <td>0.701245</td>\n",
       "      <td>3372.310059</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1215.458252</td>\n",
       "      <td>1.096235</td>\n",
       "      <td>481.723999</td>\n",
       "      <td>1109.954224</td>\n",
       "      <td>-0.782542</td>\n",
       "      <td>3.171629</td>\n",
       "      <td>114.879997</td>\n",
       "      <td>0.388219</td>\n",
       "      <td>0.230745</td>\n",
       "      <td>3487.645020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1315.414062</td>\n",
       "      <td>1.144087</td>\n",
       "      <td>223.345001</td>\n",
       "      <td>822.572876</td>\n",
       "      <td>-1.027882</td>\n",
       "      <td>2.595347</td>\n",
       "      <td>60.324699</td>\n",
       "      <td>0.547446</td>\n",
       "      <td>0.574635</td>\n",
       "      <td>3399.425781</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1329.180664</td>\n",
       "      <td>-0.328059</td>\n",
       "      <td>215.315994</td>\n",
       "      <td>1175.692261</td>\n",
       "      <td>1.269735</td>\n",
       "      <td>3.180389</td>\n",
       "      <td>280.858002</td>\n",
       "      <td>0.439630</td>\n",
       "      <td>0.549783</td>\n",
       "      <td>3378.046875</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1301.226807</td>\n",
       "      <td>-0.007400</td>\n",
       "      <td>83.551003</td>\n",
       "      <td>1282.781128</td>\n",
       "      <td>1.467489</td>\n",
       "      <td>3.136530</td>\n",
       "      <td>37.140400</td>\n",
       "      <td>0.748080</td>\n",
       "      <td>0.730503</td>\n",
       "      <td>3322.681152</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1732.102783</td>\n",
       "      <td>-0.537198</td>\n",
       "      <td>470.036987</td>\n",
       "      <td>1613.386963</td>\n",
       "      <td>-0.625840</td>\n",
       "      <td>3.133868</td>\n",
       "      <td>105.877998</td>\n",
       "      <td>0.143544</td>\n",
       "      <td>0.201208</td>\n",
       "      <td>3414.175293</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1659.375244</td>\n",
       "      <td>1.049365</td>\n",
       "      <td>524.687988</td>\n",
       "      <td>1639.449463</td>\n",
       "      <td>0.290613</td>\n",
       "      <td>3.076501</td>\n",
       "      <td>132.184006</td>\n",
       "      <td>0.157061</td>\n",
       "      <td>0.495402</td>\n",
       "      <td>3606.764893</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pt1      eta1          m1          pt2      eta2      phi2  \\\n",
       "0      1445.309570 -0.370347   57.344601  1356.463745  1.144163  3.145604   \n",
       "1      1626.900635  0.364539  103.980003  1536.651489 -0.616633  3.125390   \n",
       "2      1752.570679 -0.254160  480.951996  1661.777588 -0.784472  3.216936   \n",
       "3      1547.765137 -0.299859  205.778000  1436.573853  0.674742  3.127375   \n",
       "4      1215.458252  1.096235  481.723999  1109.954224 -0.782542  3.171629   \n",
       "...            ...       ...         ...          ...       ...       ...   \n",
       "99995  1315.414062  1.144087  223.345001   822.572876 -1.027882  2.595347   \n",
       "99996  1329.180664 -0.328059  215.315994  1175.692261  1.269735  3.180389   \n",
       "99997  1301.226807 -0.007400   83.551003  1282.781128  1.467489  3.136530   \n",
       "99998  1732.102783 -0.537198  470.036987  1613.386963 -0.625840  3.133868   \n",
       "99999  1659.375244  1.049365  524.687988  1639.449463  0.290613  3.076501   \n",
       "\n",
       "               m2   tau21j1   tau21j2          mjj  sblabel  \n",
       "0      389.571991  0.589472  0.445159  3677.823730      0.0  \n",
       "1      384.050995  0.522831  0.283367  3593.491455      0.0  \n",
       "2      107.120003  0.570297  0.358381  3605.607422      1.0  \n",
       "3      235.145004  0.826377  0.701245  3372.310059      0.0  \n",
       "4      114.879997  0.388219  0.230745  3487.645020      0.0  \n",
       "...           ...       ...       ...          ...      ...  \n",
       "99995   60.324699  0.547446  0.574635  3399.425781      0.0  \n",
       "99996  280.858002  0.439630  0.549783  3378.046875      0.0  \n",
       "99997   37.140400  0.748080  0.730503  3322.681152      0.0  \n",
       "99998  105.877998  0.143544  0.201208  3414.175293      1.0  \n",
       "99999  132.184006  0.157061  0.495402  3606.764893      1.0  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset = []\n",
    "for i in range(len(mixedsb)):\n",
    "    classifier_real = mixedsb[i][:,[2,6,7,8]]\n",
    "    classifier_fake = generated_data[i][:,[2,6,7,8]]\n",
    "    sblabel_real = mixedsb[i][:,10]\n",
    "    sblabel_fake = generated_data[i][:,10]\n",
    "    unscaled_data = np.concatenate((classifier_real,classifier_fake),axis=0)\n",
    "    sblabel = np.concatenate((sblabel_real,sblabel_fake),axis = 0)\n",
    "    scaler = StandardScaler().fit(unscaled_data)\n",
    "    scaled_data = scaler.transform(unscaled_data)\n",
    "    sblabel = sblabel.reshape(len(sblabel),1)\n",
    "    rflabels = np.concatenate((np.ones([len(classifier_real),1]),np.zeros([len(classifier_fake),1])),axis=0)\n",
    "    scaled_data = np.concatenate((scaled_data,sblabel),axis=1)\n",
    "    scaled_dataset.append(np.concatenate((scaled_data,rflabels),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 11)\n",
      "(100000, 11)\n",
      "(200000, 6)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(generated_data[0].shape)\n",
    "print(mixedsb[0].shape)\n",
    "print(scaled_dataset[0].shape)\n",
    "print(len(scaled_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dataset_label_df = pd.DataFrame(scaled_dataset[4], columns = ['m1','m2','tau21j1','tau21j2','sblabel','rflabel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>tau21j1</th>\n",
       "      <th>tau21j2</th>\n",
       "      <th>sblabel</th>\n",
       "      <th>rflabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.347627</td>\n",
       "      <td>-0.680353</td>\n",
       "      <td>0.005971</td>\n",
       "      <td>0.815416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.236394</td>\n",
       "      <td>-0.691614</td>\n",
       "      <td>-0.221293</td>\n",
       "      <td>0.215253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.337261</td>\n",
       "      <td>-1.256479</td>\n",
       "      <td>-0.059423</td>\n",
       "      <td>0.493516</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.993591</td>\n",
       "      <td>-0.995342</td>\n",
       "      <td>0.813880</td>\n",
       "      <td>1.765356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.335420</td>\n",
       "      <td>-1.240650</td>\n",
       "      <td>-0.680357</td>\n",
       "      <td>0.020055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>0.412451</td>\n",
       "      <td>0.964793</td>\n",
       "      <td>0.495351</td>\n",
       "      <td>-0.830339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0.796083</td>\n",
       "      <td>0.966752</td>\n",
       "      <td>-1.591663</td>\n",
       "      <td>-0.830339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>0.904905</td>\n",
       "      <td>0.967646</td>\n",
       "      <td>0.843778</td>\n",
       "      <td>-0.830339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>1.417099</td>\n",
       "      <td>0.967525</td>\n",
       "      <td>-0.095409</td>\n",
       "      <td>-0.830339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>1.031786</td>\n",
       "      <td>0.966912</td>\n",
       "      <td>0.933726</td>\n",
       "      <td>-0.830339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              m1        m2   tau21j1   tau21j2  sblabel  rflabel\n",
       "0      -1.347627 -0.680353  0.005971  0.815416      0.0      1.0\n",
       "1      -1.236394 -0.691614 -0.221293  0.215253      0.0      1.0\n",
       "2      -0.337261 -1.256479 -0.059423  0.493516      1.0      1.0\n",
       "3      -0.993591 -0.995342  0.813880  1.765356      0.0      1.0\n",
       "4      -0.335420 -1.240650 -0.680357  0.020055      0.0      1.0\n",
       "...          ...       ...       ...       ...      ...      ...\n",
       "199995  0.412451  0.964793  0.495351 -0.830339      0.0      0.0\n",
       "199996  0.796083  0.966752 -1.591663 -0.830339      0.0      0.0\n",
       "199997  0.904905  0.967646  0.843778 -0.830339      0.0      0.0\n",
       "199998  1.417099  0.967525 -0.095409 -0.830339      0.0      0.0\n",
       "199999  1.031786  0.966912  0.933726 -0.830339      0.0      0.0\n",
       "\n",
       "[200000 rows x 6 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dataset_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/10\n",
      "160000/160000 [==============================] - 9s 58us/sample - loss: 0.0100 - tp: 79863.0000 - fp: 30.0000 - tn: 79963.0000 - fn: 144.0000 - accuracy: 0.9989 - precision: 0.9996 - recall: 0.9982 - auc: 1.0000 - val_loss: 1.7760e-05 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 2/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 7.0630e-05 - tp: 80004.0000 - fp: 2.0000 - tn: 79991.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.0721e-06 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 3/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 5.0280e-05 - tp: 80005.0000 - fp: 2.0000 - tn: 79991.0000 - fn: 2.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.2169e-05 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 4/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 3.4898e-06 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.5938e-07 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 6.5759e-06 - tp: 80006.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.6378e-08 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 1.5331e-04 - tp: 80004.0000 - fp: 2.0000 - tn: 79991.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.0524e-07 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 7.2860e-07 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.7006e-07 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 1.0353e-05 - tp: 80006.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.3845e-05 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 3.1156e-05 - tp: 80006.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 1.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.0256e-08 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 9.2216e-07 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.3699e-10 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/10\n",
      "160000/160000 [==============================] - 7s 47us/sample - loss: 0.0084 - tp: 79823.0000 - fp: 11.0000 - tn: 79982.0000 - fn: 184.0000 - accuracy: 0.9988 - precision: 0.9999 - recall: 0.9977 - auc: 1.0000 - val_loss: 1.0948e-04 - val_tp: 19990.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 3.0000 - val_accuracy: 0.9999 - val_precision: 1.0000 - val_recall: 0.9998 - val_auc: 1.0000\n",
      "Epoch 2/10\n",
      "160000/160000 [==============================] - 6s 35us/sample - loss: 1.3661e-04 - tp: 80002.0000 - fp: 2.0000 - tn: 79991.0000 - fn: 5.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9999 - auc: 1.0000 - val_loss: 2.0147e-05 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 3/10\n",
      "160000/160000 [==============================] - 6s 35us/sample - loss: 6.2707e-06 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.9000e-06 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 4/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 1.2269e-06 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.1159e-06 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/10\n",
      "160000/160000 [==============================] - 6s 35us/sample - loss: 7.8756e-07 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.9716e-07 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/10\n",
      "160000/160000 [==============================] - 6s 35us/sample - loss: 1.0314e-04 - tp: 80004.0000 - fp: 1.0000 - tn: 79992.0000 - fn: 3.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.5286e-07 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/10\n",
      "160000/160000 [==============================] - 6s 35us/sample - loss: 4.5619e-07 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.2195e-07 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/10\n",
      "160000/160000 [==============================] - 6s 35us/sample - loss: 4.2025e-07 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.6923e-08 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 1.0579e-07 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.1381e-08 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/10\n",
      "160000/160000 [==============================] - 6s 36us/sample - loss: 9.8691e-08 - tp: 80007.0000 - fp: 0.0000e+00 - tn: 79993.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.1538e-08 - val_tp: 19993.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/10\n",
      "160000/160000 [==============================] - 7s 46us/sample - loss: 0.0080 - tp: 79985.0000 - fp: 88.0000 - tn: 79905.0000 - fn: 22.0000 - accuracy: 0.9993 - precision: 0.9989 - recall: 0.9997 - auc: 1.0000 - val_loss: 8.0981e-05 - val_tp: 19991.0000 - val_fp: 0.0000e+00 - val_tn: 20007.0000 - val_fn: 2.0000 - val_accuracy: 0.9999 - val_precision: 1.0000 - val_recall: 0.9999 - val_auc: 1.0000\n",
      "Epoch 2/10\n",
      "102784/160000 [==================>...........] - ETA: 1s - loss: 8.6611e-05 - tp: 51382.0000 - fp: 0.0000e+00 - tn: 51398.0000 - fn: 4.0000 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9999 - auc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-40e2f9dc52e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msby_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msby_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaled_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlearningCurveLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/miniconda2/envs/ad/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ypred4c = []\n",
    "fp_4c, tp_4c,th_4c= [],[],[]\n",
    "auc_list_4c = []\n",
    "for i in range(len(mixedsb)): \n",
    "    x_train,x_val,y_train,y_val = train_test_split(scaled_dataset[i][:,0:4],scaled_dataset[i][:,-1], test_size = 0.2, random_state=42)\n",
    "    _,_,sby_train,sby_val = train_test_split(scaled_dataset[i][:,0:4],scaled_dataset[i][:,-2], test_size = 0.2, random_state=42)\n",
    "    classifier = classifier_model()\n",
    "    history = classifier.fit(x_train, y_train, epochs=10, batch_size=128,validation_data=(x_val,y_val))\n",
    "    learningCurveLoss(history)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    ypred4c.append(y_pred)\n",
    "    fpr, tpr, thresholds = roc_curve(sby_test, y_pred)\n",
    "    auc_value = auc(fpr,tpr)\n",
    "    auc_list_4c.append(auc_value)\n",
    "    fp_4c.append(fpr)\n",
    "    tp_4c.append(tpr)\n",
    "    th_4c.append(thresholds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
